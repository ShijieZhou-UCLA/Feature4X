# Authors: Hui Ren (rhfeiyang.github.io)
import torch
import os, os.path as osp
import sys
import logging
import numpy as np
from lib_4d.camera import SimpleFovCamerasIndependent
from lib_4d.gs_static_model import StaticGaussian
from lib_4d.gs_ed_model import DynSCFGaussian
import imageio
from omegaconf import OmegaConf
from lib_4d.render_helper import GS_BACKEND
from lib_4d.figure_viz_helper import *

import random
import time 
import requests 
import json 
import cv2, base64

logging.getLogger("imageio_ffmpeg").setLevel(logging.ERROR)

def gptv_reflection_prompt_selectbest(user_prompt, img_prompt, idea_transcript, listofimages, args):
    num_img = len(listofimages)
    transcript = [{ "role": "system", "content": [] }, {"role": "user", "content": []}]
    # System prompt
    transcript[0]["content"].append("You are a helpful assistant.\n\nYou are a judge to rank provided images. Below are %d images generated by an AI art generation model, indexed from 0 to %d."%(num_img,num_img-1))
    transcript[0]["content"].append("From scale 1 to 10, decide how similar each image is to the user imagined IDEA of the scene.")
    transcript[-1]["content"] = transcript[-1]["content"] + idea_transcript
    for img_i in range(num_img):
        transcript[-1]["content"].append("%d. "%img_i)
        transcript[-1]["content"].append(load_img(listofimages[img_i]))

    transcript[-1]["content"].append("Let's think step by step. Check all aspects to see how well these images strictly follow the content in IDEA, including having correct object counts, attributes, entities, relationships, sizes, appearance, and all other descriptions in the IDEA. Then give a score for each input images. Finally, consider the scores and select the image with the best overall quality with image index 0 to %d wrapped with <START> and <END>. Only wrap single image index digits between <START> and <END>."%(num_img-1))

    response = gptv_query(transcript)
    if '<START>' not in response or '<END>' not in response: ## one format retry
        response = gptv_query(transcript, temp=0.1)
    if args.verbose:
        print('gptv_reflection_prompt_selectbest\n %s\n'%(response))
    if '<START>' not in response or '<END>' not in response:
        return random.randint(0,num_img-1), response
    prompts = response.split('<START>')[1]
    prompts = prompts.strip().split('<END>')[0]
    return int(prompts) if prompts.isdigit() else random.randint(0,num_img-1), response

def gptv_query(transcript=None, temp=0.):
    max_completion_tokens = 1500 #512
    wait_time = 10

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    }
    data = {
        'model': 'gpt-4o-mini',
        'max_completion_tokens': max_completion_tokens, 
        'temperature': temp,
        'top_p': 0.5,
        'messages':[]
    }
    if transcript is not None:
        data['messages'] = transcript

    response_text, retry, response_json = '', 0, None
    while len(response_text)<2:
        retry += 1
        try:
            response = requests.post("https://api.openai.com/v1/chat/completions", headers=headers, data=json.dumps(data)) 
            response_json = response.json()
        except Exception as e:
            if random.random()<1: print(e)
            time.sleep(wait_time)
            continue
        if response.status_code != 200:
            print(response.headers,response.content)
            if random.random()<0.01: print(f"The response status code for is {response.status_code} (Not OK)")
            time.sleep(wait_time)
            data['temperature'] = min(data['temperature'] + 0.2, 1.0)
            continue
        if 'choices' not in response_json:
            time.sleep(wait_time)
            continue
        response_text = response_json["choices"][0]["message"]["content"]
    return response_json["choices"][0]["message"]["content"]

def encode_image(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

def load_img(image_path):
    base64_image = encode_image(image_path)
    image_meta = "data:image/png;base64" if 'png' in image_path else "data:image/jpeg;base64"
    img_dict = {
        "type": "image_url",
        "image_url": {
          "url": f"{image_meta},{base64_image}",
          "detail": "low"
        }
    }
    return img_dict

def gptv_init_prompt(user_prompt, img_prompt, idea_transcript, args):
    transcript = [{ "role": "system", "content": [] }, {"role": "user", "content": []}]
    # System prompt
    transcript[0]["content"].append("You are a helpful assistant.\n\nInstruction: Given a user imagined IDEA of the scene, converting the IDEA into the following format. If user says they want to change color, then operation should be color_func and nothing else. If user says they want to extract an object, then operation should be extraction and nothing else. If user says they want to delete an object, then operation should be deletion and nothing else.\n")
    transcript[0]["content"].append("edit: \n ")
    transcript[0]["content"].append(', '.join(["objects:" + "["+"sky", "grass", "tree", "ground", "car" + "] \n"]))
    transcript[0]["content"].append("operations:" + "deletion/extraction/color_func" + "\n")
    transcript[0]["content"].append("targets:" +  "car" + "\n" + "threshold: 0.191 \n")
    transcript[0]["content"].append("""function: lambda color: torch.stack([color[:, 0].masked_fill(color[:, 0] >= -0.7, assign_values[0]),color[:, 1].masked_fill(color[:, 1] >= -0.7, assign_values[1]),color[:, 2].masked_fill(color[:, 2] >= -0.7, assign_values[2])], dim=1) if mode == "assign" else ((color + torch.Tensor([adjustment]).to(color)) if mode == "additive" else color * adjustment)
                                    # This function supports three types of transformations:
                                    # 1. **Direct Assignment (default, mode="assign")** - Overrides color values based on the fixed condition (color >= -0.7).
                                    # 2. **Additive/Subtractive (mode="additive")** - Adjusts RGB values by adding/subtracting a tensor.
                                    # 3. **Multiplicative Scaling (mode="multiplicative")** - Adjusts brightness/contrast by scaling color values.
                                    # The condition `color >= -0.7` is fixed for assignments to maintain color transformation stability.
                                    # `mode="assign"` (Used for Specific Colors) Example:
                                    # lambda color: torch.stack([color[:, 0].masked_fill(color[:, 0] >= -0.7, R), color[:, 1].masked_fill(color[:, 1] >= -0.7, G),color[:, 2].masked_fill(color[:, 2] >= -0.7, B)], dim=1)
                                    # R ∈ [-2, 4], -2 is black, 4 is bright red
                                    # G ∈ [-2, 4], -2 is black, 4 is bright green
                                    # B ∈ [-2, 4], -2 is black, 4 is bright blue
                                    # `mode="additive"` (Automatically Applied for "More/Less Color") Examples:
                                    # Adjust Red: lambda color: (color + torch.Tensor([[R, 0, 0]]).to(color))  # R ∈ [-4, 4], positive adds red, negative removes red
                                    # Adjust Green: lambda color: (color + torch.Tensor([[0, G, 0]]).to(color))  # G ∈ [-4, 4], positive adds green, negative removes green
                                    # Adjust Blue: lambda color: (color + torch.Tensor([[0, 0, B]]).to(color))  # B ∈ [-4, 4], positive adds blue, negative removes blue
                                    # Adjust Brightness: lambda color: (color + torch.Tensor([[V, V, V]]).to(color))  # V ∈ [-4, 4], positive brightens, negative darkens
                                    # Reduce a Specific Color: lambda color: (color - torch.Tensor([[R, G, B]]).to(color))  # Explicit RGB reduction
                                    # `mode="multiplicative"` Examples:
                                    # Darken: lambda color: color * S  # S ∈ [-1, 1]
                                    # Brighten: lambda color: color * S  # S > 1
                                    # Adjust Saturation: lambda color: color * S + (1 - S) * torch.mean(color, dim=-1, keepdim=True)  # S ∈ [0,1], lower reduces saturation
                                    # Convert to Grayscale (Black and white): lambda color: torch.mean(color, dim=-1, keepdim=True).expand_as(color) 
                                    # Additional Notes:
                                    # - **Default mode is `assign` for setting user-specified colors.**
                                    # - **Use `mode="additive"` for small color agjustment.**
                                    # - `"additive"` mode is **automatically used** for **"more/less [color]"**.
                                    # - **Use `mode="multiplicative"` for contrast and brightness control.**
                                    # - **Keep values within valid ranges to avoid unwanted artifacts.**
                                    # - **Mix different transformations for complex effects.**""")

    transcript[0]["content"] = ' '.join(transcript[0]["content"])
    ## Example & Query prompt
    print('transciprt content', transcript[-1]["content"])
    transcript[-1]["content"] = transcript[-1]["content"] + idea_transcript
    transcript[-1]["content"].append("Based on the above information, you will write %d prompts exactly about the IDEA follow the rules with different threshold values from %.3f to %.3f, and the potentially required color_func. Each prompt is wrapped with <START> and <END>.\n"% (args.num_attempt, args.threshold_lb, args.threshold_ub))
    transcript[-1]["content"] = ' '.join(transcript[-1]["content"])

    print('transcript', transcript)
    response = gptv_query(transcript)
    if '<START>' not in response or '<END>' not in response: 
        response = gptv_query(transcript, temp=0.1)
    if args.verbose:
        print('gptv_init_prompt    IDEA: %s.\n %s\n'%(user_prompt, response))
    prompts = response.split('<START>')[1:]
    prompts = [x.strip().split('<END>')[0] for x in prompts]
    return prompts

def load_model_cfg(cfg, log_dir, device=torch.device("cuda")):

    # get cfg
    if log_dir.endswith("/"):
        log_dir = log_dir[:-1]
    if isinstance(cfg, str):
        cfg = OmegaConf.load(cfg)
        OmegaConf.set_readonly(cfg, True)

    dataset_mode = getattr(cfg, "dataset_mode", "iphone")
    max_sph_order = getattr(cfg, "max_sph_order", 1)
    logging.info(f"Dataset mode: {dataset_mode}")

    d_model_ckpt = torch.load(osp.join(log_dir, "finetune_d_model.pth"), weights_only=True)
    d_model = DynSCFGaussian.load_from_ckpt(d_model_ckpt, device=device)
    # * load static model and camera again, because it's also finetuned
    saved_cam = torch.load(osp.join(log_dir, "finetune_s_model_cam.pth"), weights_only=True)
    cams: SimpleFovCamerasIndependent = SimpleFovCamerasIndependent(
        T=len(saved_cam["q_wc"]),
        fovdeg_init=40.0,  # dummy init
    )
    cams.load_state_dict(saved_cam, strict=True)

    s_model = StaticGaussian(
        load_fn=osp.join(log_dir, "finetune_s_model.pth"),
        max_sph_order=max_sph_order,
    ).to(device)

    cams.to(device)
    cams.eval()
    d_model.to(device)
    d_model.eval()
    s_model.to(device)
    s_model.eval()

    # feature_head
    feature_config = osp.join(log_dir, "feature_config.yaml")
    if osp.exists(feature_config):
        with open(feature_config, "r") as f:
            feature_config = OmegaConf.load(f)
            head_config = feature_config["Head"]

        feature_head = Feature_heads(head_config).to(device)
        feature_head_ckpt_path = osp.join(log_dir, "finetune_semantic_heads.pth")
        feature_head_state = torch.load(feature_head_ckpt_path, weights_only=True)
        feature_head.load_state_dict(feature_head_state)
        feature_head.eval()

    else:
        print(f"feature head config {feature_config} not found")
        feature_head = None

    return cfg, d_model, s_model, cams, feature_head


@torch.no_grad()
def viz_main(
    save_dir,
    log_dir,
    cfg_fn,
    N=1,
    H=480,
    W=854,
    move_angle_deg=4.5, #10.0,
    H_3d=960,
    W_3d=960,
    fov_3d=70,
    gptv_prompts=None,
    args=None,
    round_best=None,
):
    os.makedirs(save_dir, exist_ok = True)

    cfg, d_model, s_model, cams, feature_head = load_model_cfg(cfg_fn, log_dir)

    rel_focal_3d = 1.0 / np.tan(np.deg2rad(fov_3d) / 2.0)

    key_steps = [cams.T // 2, cams.T - 1, 0, cams.T // 4, 3 * cams.T // 4][:N]

    # * Get pose
    global_pose_list = get_global_3D_cam_T_cw(
        s_model,
        d_model,
        cams,
        H,
        W,
        cams.T // 2,
        back_ratio=0.5,
        up_ratio=0.2,
    )
    global_pose_list = global_pose_list[None].expand(cams.T, -1, -1)
    training_pose_list = [cams.T_cw(t) for t in range(cams.T)]

    # viz 3D
    save_fn_prefix = osp.join(save_dir, f"3D_moving")
    viz_single_2d_video_agent(
        H_3d,
        W_3d,
        cams,
        s_model,
        d_model,
        save_fn_prefix,
        global_pose_list,
        rel_focal=rel_focal_3d,
        gptv_prompts=gptv_prompts,
        args=args,
        round_best=round_best,
        feature_head=feature_head
    )

    for key_time_step in key_steps:
        fixed_pose_list = [cams.T_cw(key_time_step) for _ in range(cams.T)]
        round_pose_list = get_move_around_cam_T_cw(
            s_model,
            d_model,
            cams,
            H,
            W,
            np.deg2rad(move_angle_deg),
            total_steps=cams.T,  # cams.T
            center_id=key_time_step,
        )

        # Viz rgb
        save_fn_prefix = osp.join(save_dir, f"{key_time_step}_round_moving")
        logging.info(f"Vizing round_moving")
        viz_single_2d_video_agent(
            H, W, cams, s_model, d_model, save_fn_prefix, round_pose_list, args=args, gptv_prompts=gptv_prompts, round_best=round_best, feature_head=feature_head
        )

    return


if __name__ == "__main__":
    import argparse
    import os
    args = argparse.ArgumentParser()
    args.add_argument("--config", "-c", type=str, required=True)
    args.add_argument("--root", "-r", type=str, required=True)
    args.add_argument("--api_key", type=str, help="OpenAI API key (or set OPENAI_API_KEY environment variable)")
    args.add_argument("--N", "-n", type=int, default=1)
    args.add_argument("--H", type=int, default=480)
    args.add_argument("--W", type=int, default=480)
    args.add_argument("--user_prompt", type=str, default="change the dog color to blue.")
    ### hyper-params ###
    args.add_argument("--num_attempt", type=int, default=10)
    args.add_argument("--threshold_lb", type=float, default=0.85)
    args.add_argument("--threshold_ub", type=float, default=0.95)
    ###
    args = args.parse_args()
    
    # Handle API key - check environment variable if not provided
    if not args.api_key:
        args.api_key = os.getenv('OPENAI_API_KEY')
        if args.api_key:
            print("✅ Using OPENAI_API_KEY from environment variables")
        else:
            print("❌ OpenAI API key required!")
            print("💡 Setup Options:")
            print("   1. Set environment variable: export OPENAI_API_KEY=your_api_key_here")
            print("   2. Add to your ~/.bashrc: echo 'export OPENAI_API_KEY=your_api_key_here' >> ~/.bashrc")
            print("   3. Or provide via command line: --api_key your_api_key_here")
            print("")
            print("Get your API key at: https://platform.openai.com/account/api-keys")
            sys.exit(1)
    else:
        print("✅ Using provided OpenAI API key")
    args.save = os.path.join(args.root,"editing_output",args.user_prompt.replace(' ','').replace('.',''))
    args.verbose = True 
    args.semantic_head_path = os.path.join(args.root,"finetune_semantic_heads.pth")
    args.foldername = "agentic_edit_record"

    os.system('mkdir -p %s/%s'%(args.root, args.foldername))
    os.system('mkdir -p %s/%s'%(args.root, args.foldername))
    os.system('mkdir -p %s/%s/iter'%(args.root, args.foldername))
    os.system('mkdir -p %s/%s/round1'%(args.root, args.foldername))
    os.system('mkdir -p %s/%s/iter_best'%(args.root, args.foldername))
    os.system('mkdir %s/%s/tmp'%(args.root, args.foldername))

    api_key = args.api_key
    user_prompt = args.user_prompt
    img_prompt = None
    prompt_list = user_prompt.split('<IMG>')
    user_prompt = user_prompt.split('<IMG>')[0]
    idea_transcript = []
    for ii in range(len(prompt_list)):
        if ii == 0:
            idea_transcript.append("IDEA: %s."%prompt_list[0])
        elif ii%2==1:
            idea_transcript.append(load_img(prompt_list[ii]))
        elif ii%2==0:
            idea_transcript.append("%s"%prompt_list[ii])
    idea_transcript.append("End of IDEA.\n")

    idea_transcript = [idea_transcript[0] + idea_transcript[1]]
    print('idea transcirpt ===', idea_transcript)

    ### GPTV prompting iter
    current_prompts, prompt_history, select_history, image_history, reflection_history, bestidx_history = [],[],[],[],[],[]
    gptv_prompts =[]

    os.system('mkdir %s/%s/tmp/%s'%(args.root, args.foldername, user_prompt.replace(' ','').replace('.','')))

    for rounds in range(1):
        print('ROUND %d:\n'%rounds)
        if rounds == 0:
            gptv_prompts = gptv_init_prompt(user_prompt, None, idea_transcript, args)
        current_prompts = gptv_prompts
        print('gptv_prompts', gptv_prompts)
        for ii in range(args.num_attempt):
            print('threshold tuning', ii)
            print(gptv_prompts[ii])

            args.save_name = '%s/%s/tmp/%s/%d_%d_%d.png'%(args.root, args.foldername,user_prompt.replace(' ','').replace('.',''),rounds,ii,0)

            viz_main(
                args.save,
                args.root,
                args.config,
                N=1,
                H=480,
                W=480,
                gptv_prompts=gptv_prompts[ii],
                args=args
            )

    round_best, select_response = gptv_reflection_prompt_selectbest(user_prompt, img_prompt, idea_transcript, ['%s/%s/tmp/%s/%d_%d_0.png'%(args.root, args.foldername,user_prompt.replace(' ','').replace('.',''), rounds, ii) for ii in range(args.num_attempt)], args)
    print('For your reference, the best round is', round_best)

